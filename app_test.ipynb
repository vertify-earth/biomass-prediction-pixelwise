{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Optimized Biomass Prediction Pipeline for Gradio Interface\n",
    "# \n",
    "# This notebook contains an optimized version of the biomass prediction pipeline\n",
    "# specifically designed for processing smaller GeoTIFF chips within a Gradio interface.\n",
    "# \n",
    "# Author: najahpokkiri\n",
    "# Date: 2025-05-17\n",
    "\n",
    "# ## 1. Import Libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Geospatial libraries\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "# Data processing\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from io import BytesIO\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check if scikit-image is available for texture features\n",
    "try:\n",
    "    from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "    from skimage.filters import sobel\n",
    "    SKIMAGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: scikit-image not available. Texture features will be disabled.\")\n",
    "    SKIMAGE_AVAILABLE = False\n",
    "\n",
    "# ## 2. Define Model Architecture\n",
    "\n",
    "class StableResNet(nn.Module):\n",
    "    \"\"\"Numerically stable ResNet for biomass regression\"\"\"\n",
    "    def __init__(self, n_features, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.layer1 = self._make_simple_resblock(256, 256)\n",
    "        self.layer2 = self._make_simple_resblock(256, 128)\n",
    "        self.layer3 = self._make_simple_resblock(128, 64)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def _make_simple_resblock(self, in_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.ReLU()\n",
    "        ) if in_dim == out_dim else nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        identity = x\n",
    "        out = self.layer1(x)\n",
    "        x = out + identity\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.regressor(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# ## 3. Model Loading Functions\n",
    "\n",
    "def load_model(model_dir, device=None):\n",
    "    \"\"\"\n",
    "    Load the biomass prediction model and associated metadata.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory containing model files\n",
    "        device (torch.device): Device to load the model on\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing model, scaler, and metadata\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    result = {\n",
    "        'model': None,\n",
    "        'package': None,\n",
    "        'feature_names': None,\n",
    "        'metadata': {\n",
    "            'loaded_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'device': str(device)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load feature names\n",
    "        feature_path = os.path.join(model_dir, \"feature_names.txt\")\n",
    "        if os.path.exists(feature_path):\n",
    "            with open(feature_path, 'r') as f:\n",
    "                result['feature_names'] = [line.strip() for line in f if line.strip()]\n",
    "            print(f\"Loaded {len(result['feature_names'])} feature names\")\n",
    "        else:\n",
    "            print(f\"Feature names file not found at {feature_path}\")\n",
    "            result['feature_names'] = []\n",
    "        \n",
    "        # Set number of features\n",
    "        n_features = len(result['feature_names']) if result['feature_names'] else 99\n",
    "        \n",
    "        # Load model package\n",
    "        package_path = os.path.join(model_dir, \"model_package.pkl\")\n",
    "        if os.path.exists(package_path):\n",
    "            result['package'] = joblib.load(package_path)\n",
    "            print(f\"Model package loaded from {package_path}\")\n",
    "            \n",
    "            # Update n_features if available in package\n",
    "            if 'n_features' in result['package']:\n",
    "                n_features = result['package']['n_features']\n",
    "                print(f\"Using n_features={n_features} from package\")\n",
    "        else:\n",
    "            print(f\"Model package not found at {package_path}\")\n",
    "        \n",
    "        # Load model weights\n",
    "        model_path = os.path.join(model_dir, \"model.pt\")\n",
    "        if os.path.exists(model_path):\n",
    "            model = StableResNet(n_features=n_features)\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            result['model'] = model\n",
    "            print(f\"Model loaded successfully from {model_path}\")\n",
    "        else:\n",
    "            print(f\"Model file not found at {model_path}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return result\n",
    "\n",
    "# ## 4. Feature Engineering Pipeline\n",
    "\n",
    "def safe_divide(a, b, fill_value=0.0):\n",
    "    \"\"\"Safe division that handles zeros in the denominator\"\"\"\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    b = np.asarray(b, dtype=np.float32)\n",
    "    \n",
    "    # Handle NaN/Inf in inputs\n",
    "    a = np.nan_to_num(a, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    b = np.nan_to_num(b, nan=1e-10, posinf=1e10, neginf=-1e10)\n",
    "    \n",
    "    if a.ndim == 0 and b.ndim > 0:\n",
    "        a = np.full_like(b, a)\n",
    "    elif b.ndim == 0 and a.ndim > 0:\n",
    "        b = np.full_like(a, b)\n",
    "    elif a.ndim == 0 and b.ndim == 0:\n",
    "        if abs(b) < 1e-10:\n",
    "            return fill_value\n",
    "        else:\n",
    "            return float(a / b)\n",
    "    \n",
    "    mask = np.abs(b) < 1e-10\n",
    "    result = np.full_like(a, fill_value, dtype=np.float32)\n",
    "    if np.any(~mask):\n",
    "        result[~mask] = a[~mask] / b[~mask]\n",
    "    \n",
    "    result = np.nan_to_num(result, nan=fill_value, posinf=fill_value, neginf=fill_value)\n",
    "    return result\n",
    "\n",
    "def extract_features(satellite_data, use_advanced_indices=True, \n",
    "                    use_texture_features=True, use_spatial_features=True,\n",
    "                    use_pca_features=True, pca_components=25):\n",
    "    \"\"\"\n",
    "    Extract all features from satellite data in a single function.\n",
    "    Optimized for memory efficiency and smaller GeoTIFF files.\n",
    "    \n",
    "    Args:\n",
    "        satellite_data (numpy.ndarray): Satellite image with shape (bands, height, width)\n",
    "        use_advanced_indices (bool): Whether to calculate spectral indices\n",
    "        use_texture_features (bool): Whether to extract texture features\n",
    "        use_spatial_features (bool): Whether to calculate spatial features\n",
    "        use_pca_features (bool): Whether to calculate PCA features\n",
    "        pca_components (int): Number of PCA components\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of feature arrays with shape (height, width)\n",
    "    \"\"\"\n",
    "    # Start time\n",
    "    start_time = time.time()\n",
    "    print(\"Extracting features...\")\n",
    "    \n",
    "    # Get dimensions\n",
    "    n_bands, height, width = satellite_data.shape\n",
    "    print(f\"Image dimensions: {width}x{height}, {n_bands} bands\")\n",
    "    \n",
    "    # Initialize features dictionary\n",
    "    all_features = {}\n",
    "    \n",
    "    # 1. Add original bands\n",
    "    for i in range(n_bands):\n",
    "        band_data = satellite_data[i].copy()\n",
    "        band_data = np.nan_to_num(band_data, nan=0.0)\n",
    "        all_features[f'Band_{i+1:02d}'] = band_data\n",
    "    \n",
    "    # 2. Calculate spectral indices\n",
    "    if use_advanced_indices:\n",
    "        indices_start = time.time()\n",
    "        \n",
    "        # Enhanced band mapping with error checking\n",
    "        def safe_get_band(idx):\n",
    "            return satellite_data[idx] if idx < n_bands else None\n",
    "        \n",
    "        # Try to get commonly used bands\n",
    "        blue = safe_get_band(1)  # Adjust indices based on your data\n",
    "        green = safe_get_band(2)\n",
    "        red = safe_get_band(3)\n",
    "        nir = safe_get_band(7)\n",
    "        swir1 = safe_get_band(9)\n",
    "        swir2 = safe_get_band(10)\n",
    "        \n",
    "        # Calculate indices if bands are available\n",
    "        if all(b is not None for b in [red, nir]):\n",
    "            # NDVI (Normalized Difference Vegetation Index)\n",
    "            all_features['NDVI'] = safe_divide(nir - red, nir + red)\n",
    "            \n",
    "            if blue is not None and green is not None:\n",
    "                # EVI (Enhanced Vegetation Index)\n",
    "                all_features['EVI'] = 2.5 * safe_divide(nir - red, nir + 6*red - 7.5*blue + 1)\n",
    "                \n",
    "                # SAVI (Soil Adjusted Vegetation Index)\n",
    "                all_features['SAVI'] = 1.5 * safe_divide(nir - red, nir + red + 0.5)\n",
    "                \n",
    "                # MSAVI2 (Modified Soil Adjusted Vegetation Index)\n",
    "                all_features['MSAVI2'] = 0.5 * (2 * nir + 1 - np.sqrt((2 * nir + 1)**2 - 8 * (nir - red)))\n",
    "                \n",
    "                # NDWI (Normalized Difference Water Index)\n",
    "                all_features['NDWI'] = safe_divide(green - nir, green + nir)\n",
    "        \n",
    "        if swir1 is not None and nir is not None:\n",
    "            # NDMI (Normalized Difference Moisture Index)\n",
    "            all_features['NDMI'] = safe_divide(nir - swir1, nir + swir1)\n",
    "        \n",
    "        if swir2 is not None and nir is not None:\n",
    "            # NBR (Normalized Burn Ratio)\n",
    "            all_features['NBR'] = safe_divide(nir - swir2, nir + swir2)\n",
    "        \n",
    "        indices_time = time.time() - indices_start\n",
    "        print(f\"Calculated {len(all_features) - n_bands} spectral indices in {indices_time:.2f}s\")\n",
    "    \n",
    "    # 3. Extract texture features\n",
    "    if use_texture_features and SKIMAGE_AVAILABLE:\n",
    "        texture_start = time.time()\n",
    "        \n",
    "        # Select representative bands for texture analysis (e.g., NIR bands)\n",
    "        key_bands = [7]  # NIR band\n",
    "        \n",
    "        for band_idx in key_bands:\n",
    "            if band_idx >= n_bands:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                band = satellite_data[band_idx].copy()\n",
    "                \n",
    "                # Normalize to 0-255 for texture analysis\n",
    "                band_min, band_max = np.nanpercentile(band[~np.isnan(band)], [1, 99])\n",
    "                band_norm = np.clip((band - band_min) / (band_max - band_min + 1e-8), 0, 1)\n",
    "                band_norm = (band_norm * 255).astype(np.uint8)\n",
    "                \n",
    "                # Replace NaN with median\n",
    "                band_norm = np.nan_to_num(band_norm, nan=np.nanmedian(band_norm))\n",
    "                \n",
    "                # Edge detection using Sobel\n",
    "                sobel_response = sobel(band_norm.astype(float))\n",
    "                all_features[f'Sobel_B{band_idx}'] = sobel_response\n",
    "                \n",
    "                # Local Binary Pattern (for small images only)\n",
    "                if height * width < 1000000:  # ~1MP limit\n",
    "                    try:\n",
    "                        lbp = local_binary_pattern(band_norm, 8, 1, method='uniform')\n",
    "                        all_features[f'LBP_B{band_idx}'] = lbp\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Error calculating LBP: {e}\")\n",
    "                \n",
    "                # GLCM properties - simplified approach for efficiency\n",
    "                # Calculate on a smaller representative patch for memory efficiency\n",
    "                sample_size = min(128, height, width)\n",
    "                center_y, center_x = height//2, width//2\n",
    "                offset = sample_size // 2\n",
    "                y_start = max(0, center_y - offset)\n",
    "                y_end = min(height, center_y + offset)\n",
    "                x_start = max(0, center_x - offset)\n",
    "                x_end = min(width, center_x + offset)\n",
    "                \n",
    "                patch = band_norm[y_start:y_end, x_start:x_end]\n",
    "                \n",
    "                if patch.size > 0:\n",
    "                    try:\n",
    "                        glcm = graycomatrix(patch, [1], [0], levels=256, symmetric=True, normed=True)\n",
    "                        # Extract properties as scalar values\n",
    "                        for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy']:\n",
    "                            value = float(graycoprops(glcm, prop)[0, 0])\n",
    "                            # Create 2D arrays with the scalar value\n",
    "                            all_features[f'GLCM_{prop}_B{band_idx}'] = np.full((height, width), value, dtype=np.float32)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Error calculating GLCM: {e}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing band {band_idx} for texture: {e}\")\n",
    "        \n",
    "        texture_time = time.time() - texture_start\n",
    "        texture_count = len(all_features) - n_bands - (len(all_features) - n_bands)\n",
    "        print(f\"Extracted {texture_count} texture features in {texture_time:.2f}s\")\n",
    "    \n",
    "    # 4. Calculate spatial features\n",
    "    if use_spatial_features:\n",
    "        spatial_start = time.time()\n",
    "        \n",
    "        # Key bands for spatial analysis\n",
    "        key_bands = [7]  # NIR band\n",
    "        \n",
    "        for band_idx in key_bands:\n",
    "            if band_idx < n_bands:\n",
    "                try:\n",
    "                    band = satellite_data[band_idx].copy()\n",
    "                    band = np.nan_to_num(band, nan=0.0)\n",
    "                    \n",
    "                    # Calculate gradients\n",
    "                    grad_y, grad_x = np.gradient(band)\n",
    "                    grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "                    all_features[f'Gradient_B{band_idx}'] = grad_magnitude\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error calculating spatial features for band {band_idx}: {e}\")\n",
    "        \n",
    "        # Gradient features for NDVI if available\n",
    "        if 'NDVI' in all_features:\n",
    "            try:\n",
    "                ndvi_clean = np.nan_to_num(all_features['NDVI'], nan=0.0)\n",
    "                \n",
    "                # Calculate gradients\n",
    "                grad_y, grad_x = np.gradient(ndvi_clean)\n",
    "                grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "                all_features['NDVI_gradient'] = grad_magnitude\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error calculating gradient for NDVI: {e}\")\n",
    "        \n",
    "        spatial_time = time.time() - spatial_start\n",
    "        spatial_count = len(all_features) - n_bands - (len(all_features) - n_bands) - texture_count\n",
    "        print(f\"Calculated {spatial_count} spatial features in {spatial_time:.2f}s\")\n",
    "    \n",
    "    # 5. Calculate PCA features (if image is not too large)\n",
    "    if use_pca_features and height * width < 5000000:  # ~5MP limit for memory\n",
    "        pca_start = time.time()\n",
    "        \n",
    "        # Reshape for PCA (pixels x bands)\n",
    "        bands_reshaped = satellite_data.reshape(n_bands, -1).T\n",
    "        \n",
    "        # Handle NaN values\n",
    "        valid_mask = ~np.any(np.isnan(bands_reshaped), axis=1)\n",
    "        bands_clean = bands_reshaped[valid_mask]\n",
    "        \n",
    "        if len(bands_clean) > 0:\n",
    "            try:\n",
    "                # Standardize and apply PCA\n",
    "                scaler = StandardScaler()\n",
    "                bands_scaled = scaler.fit_transform(bands_clean)\n",
    "                \n",
    "                # Limit components to avoid memory issues\n",
    "                n_components = min(pca_components, bands_scaled.shape[1], 25)\n",
    "                \n",
    "                pca = PCA(n_components=n_components)\n",
    "                pca_features = pca.fit_transform(bands_scaled)\n",
    "                \n",
    "                # Create full PCA array\n",
    "                pca_full = np.zeros((height * width, pca_features.shape[1]))\n",
    "                pca_full[valid_mask] = pca_features\n",
    "                pca_full = pca_full.reshape(height, width, pca_features.shape[1])\n",
    "                \n",
    "                # Convert to dictionary format\n",
    "                for i in range(pca_full.shape[2]):\n",
    "                    all_features[f'PCA_{i+1:02d}'] = pca_full[:, :, i]\n",
    "                \n",
    "                explained_variance = pca.explained_variance_ratio_.sum()\n",
    "                print(f\"PCA explained variance: {explained_variance:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error calculating PCA: {e}\")\n",
    "        else:\n",
    "            print(\"Warning: No valid data for PCA\")\n",
    "        \n",
    "        pca_time = time.time() - pca_start\n",
    "        pca_count = len(all_features) - n_bands - (len(all_features) - n_bands) - texture_count - spatial_count\n",
    "        print(f\"Calculated {pca_count} PCA features in {pca_time:.2f}s\")\n",
    "    \n",
    "    # Report total feature count and time\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Extracted {len(all_features)} total features in {total_time:.2f}s\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "# ## 5. Prediction Function\n",
    "\n",
    "def predict_biomass(file_path, model_result, use_advanced_indices=True, \n",
    "                   use_texture_features=True, use_spatial_features=True,\n",
    "                   use_pca_features=True, pca_components=25):\n",
    "    \"\"\"\n",
    "    Main function to predict biomass from a GeoTIFF file.\n",
    "    Optimized for smaller GeoTIFF chips.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str or BytesIO): Path to GeoTIFF file or file-like object\n",
    "        model_result (dict): Dictionary containing model, package, feature_names\n",
    "        use_advanced_indices (bool): Whether to calculate spectral indices\n",
    "        use_texture_features (bool): Whether to extract texture features\n",
    "        use_spatial_features (bool): Whether to calculate spatial features\n",
    "        use_pca_features (bool): Whether to calculate PCA features\n",
    "        pca_components (int): Number of PCA components\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing predictions, statistics, and metadata\n",
    "    \"\"\"\n",
    "    # Check if model is loaded\n",
    "    if model_result['model'] is None:\n",
    "        return {\"error\": \"Model not loaded\"}\n",
    "    \n",
    "    if not model_result['feature_names']:\n",
    "        return {\"error\": \"No feature names provided\"}\n",
    "    \n",
    "    result = {\n",
    "        \"predictions\": None,\n",
    "        \"statistics\": {},\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"file_path\": str(file_path) if isinstance(file_path, str) else \"BytesIO object\",\n",
    "            \"processing_time\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load configuration from model package\n",
    "        config = {\n",
    "            \"use_log_transform\": True,\n",
    "            \"epsilon\": 1.0\n",
    "        }\n",
    "        \n",
    "        if model_result['package'] is not None:\n",
    "            if 'use_log_transform' in model_result['package']:\n",
    "                config[\"use_log_transform\"] = model_result['package']['use_log_transform']\n",
    "            if 'epsilon' in model_result['package']:\n",
    "                config[\"epsilon\"] = model_result['package']['epsilon']\n",
    "        \n",
    "        # Open raster file\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Read metadata\n",
    "            result[\"metadata\"][\"width\"] = src.width\n",
    "            result[\"metadata\"][\"height\"] = src.height\n",
    "            result[\"metadata\"][\"crs\"] = str(src.crs)\n",
    "            result[\"metadata\"][\"transform\"] = str(src.transform)\n",
    "            result[\"metadata\"][\"bands\"] = src.count\n",
    "            \n",
    "            # Read entire image\n",
    "            image_data = src.read()\n",
    "            \n",
    "            # Create validity mask\n",
    "            valid_mask = np.all(np.isfinite(image_data), axis=0)\n",
    "            \n",
    "            if np.sum(valid_mask) == 0:\n",
    "                return {\"error\": \"No valid pixels in image\"}\n",
    "            \n",
    "            # Extract all features\n",
    "            features = extract_features(\n",
    "                image_data,\n",
    "                use_advanced_indices=use_advanced_indices,\n",
    "                use_texture_features=use_texture_features,\n",
    "                use_spatial_features=use_spatial_features,\n",
    "                use_pca_features=use_pca_features,\n",
    "                pca_components=pca_components\n",
    "            )\n",
    "            \n",
    "            # Extract features in correct order for model\n",
    "            feature_names = model_result['feature_names']\n",
    "            height, width = valid_mask.shape\n",
    "            y_indices, x_indices = np.where(valid_mask)\n",
    "            feature_matrix = np.zeros((len(y_indices), len(feature_names)), dtype=np.float32)\n",
    "            \n",
    "            print(f\"Preparing {len(y_indices)} valid pixels for prediction...\")\n",
    "            for i, feature_name in enumerate(feature_names):\n",
    "                if feature_name in features:\n",
    "                    feature_data = features[feature_name]\n",
    "                    feature_values = feature_data[y_indices, x_indices]\n",
    "                    feature_values = np.nan_to_num(feature_values, nan=0.0)\n",
    "                    feature_matrix[:, i] = feature_values\n",
    "                else:\n",
    "                    # If feature is missing, use zeros\n",
    "                    print(f\"Warning: Feature '{feature_name}' not found, using zeros\")\n",
    "                    feature_matrix[:, i] = 0.0\n",
    "            \n",
    "            # Apply scaling if available\n",
    "            if model_result['package'] is not None and 'scaler' in model_result['package']:\n",
    "                try:\n",
    "                    feature_matrix = model_result['package']['scaler'].transform(feature_matrix)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error applying scaler: {e}\")\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = np.zeros((height, width), dtype=np.float32)\n",
    "            \n",
    "            print(\"Running model inference...\")\n",
    "            with torch.no_grad():\n",
    "                # Process in batches to avoid memory issues\n",
    "                batch_size = min(10000, len(y_indices))\n",
    "                \n",
    "                for i in range(0, len(y_indices), batch_size):\n",
    "                    end_idx = min(i + batch_size, len(y_indices))\n",
    "                    batch = feature_matrix[i:end_idx]\n",
    "                    \n",
    "                    # Convert to tensor\n",
    "                    device = next(model_result['model'].parameters()).device\n",
    "                    batch_tensor = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    batch_preds = model_result['model'](batch_tensor).cpu().numpy()\n",
    "                    \n",
    "                    # Handle scalar case\n",
    "                    if not isinstance(batch_preds, np.ndarray):\n",
    "                        batch_preds = np.array([batch_preds])\n",
    "                    if batch_preds.ndim == 0:\n",
    "                        batch_preds = np.array([float(batch_preds)])\n",
    "                    \n",
    "                    # Convert from log scale if needed\n",
    "                    if config[\"use_log_transform\"]:\n",
    "                        batch_preds = np.exp(batch_preds) - config[\"epsilon\"]\n",
    "                        batch_preds = np.maximum(batch_preds, 0)  # Ensure non-negative\n",
    "                    \n",
    "                    # Map predictions back to the correct pixels\n",
    "                    for j in range(end_idx - i):\n",
    "                        y_idx = y_indices[i + j]\n",
    "                        x_idx = x_indices[i + j]\n",
    "                        predictions[y_idx, x_idx] = batch_preds[j]\n",
    "                    \n",
    "                    print(f\"\\rProcessed {end_idx}/{len(y_indices)} pixels\", end=\"\")\n",
    "            \n",
    "            print(\"\\nPrediction complete!\")\n",
    "            \n",
    "            # Calculate statistics\n",
    "            valid_predictions = predictions[valid_mask]\n",
    "            \n",
    "            result[\"statistics\"] = {\n",
    "                \"min\": float(np.min(valid_predictions)),\n",
    "                \"max\": float(np.max(valid_predictions)),\n",
    "                \"mean\": float(np.mean(valid_predictions)),\n",
    "                \"median\": float(np.median(valid_predictions)),\n",
    "                \"std\": float(np.std(valid_predictions)),\n",
    "                \"sum\": float(np.sum(valid_predictions)),\n",
    "                \"pixel_count\": int(valid_predictions.size),\n",
    "                \"valid_percentage\": float(100 * valid_predictions.size / (height * width))\n",
    "            }\n",
    "            \n",
    "            # Calculate total biomass if transform is available\n",
    "            if src.transform:\n",
    "                pixel_area_m2 = abs(src.transform[0] * src.transform[4])\n",
    "                area_hectares = valid_predictions.size * pixel_area_m2 / 10000\n",
    "                total_biomass_mg = np.sum(valid_predictions) * pixel_area_m2 / 10000\n",
    "                \n",
    "                result[\"statistics\"][\"area_hectares\"] = float(area_hectares)\n",
    "                result[\"statistics\"][\"total_biomass_mg\"] = float(total_biomass_mg)\n",
    "            \n",
    "            # Store predictions\n",
    "            result[\"predictions\"] = predictions\n",
    "            result[\"valid_mask\"] = valid_mask\n",
    "            result[\"metadata\"][\"processing_time\"] = time.time() - start_time\n",
    "            \n",
    "            print(f\"Prediction complete in {result['metadata']['processing_time']:.2f}s\")\n",
    "            print(f\"Biomass range: {result['statistics']['min']:.2f} - {result['statistics']['max']:.2f} Mg/ha\")\n",
    "            print(f\"Mean biomass: {result['statistics']['mean']:.2f} Mg/ha\")\n",
    "            \n",
    "            # Clean up\n",
    "            del features, feature_matrix\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_trace = traceback.format_exc()\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"traceback\": error_trace,\n",
    "            \"metadata\": {\n",
    "                \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"file_path\": str(file_path) if isinstance(file_path, str) else \"BytesIO object\",\n",
    "                \"processing_time\": time.time() - start_time\n",
    "            }\n",
    "        }\n",
    "\n",
    "# ## 6. Visualization Functions\n",
    "\n",
    "def create_biomass_visualization(result, visualization_type=\"heatmap\", rgb_indexes=None, dpi=100):\n",
    "    \"\"\"\n",
    "    Create visualization of biomass predictions.\n",
    "    \n",
    "    Args:\n",
    "        result (dict): Result dictionary from predict_biomass\n",
    "        visualization_type (str): Type of visualization ('heatmap' or 'rgb_overlay')\n",
    "        rgb_indexes (tuple): Indexes for RGB visualization (e.g., (3,2,1) for true color)\n",
    "        dpi (int): DPI for output image\n",
    "    \n",
    "    Returns:\n",
    "        BytesIO: Image buffer with visualization\n",
    "    \"\"\"\n",
    "    if \"error\" in result:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.text(0.5, 0.5, f\"Error: {result['error']}\", \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax.axis('off')\n",
    "        buf = BytesIO()\n",
    "        fig.savefig(buf, format='png', dpi=dpi, bbox_inches='tight')\n",
    "        buf.seek(0)\n",
    "        plt.close(fig)\n",
    "        return buf\n",
    "    \n",
    "    predictions = result[\"predictions\"]\n",
    "    valid_mask = result.get(\"valid_mask\", np.ones_like(predictions, dtype=bool))\n",
    "    \n",
    "    # Use masked array for better visualization\n",
    "    masked_predictions = np.ma.masked_where(~valid_mask, predictions)\n",
    "    \n",
    "    # Get min/max values for better visualization (1-99 percentile)\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        vmin = np.nanpercentile(predictions[valid_mask], 1)\n",
    "        vmax = np.nanpercentile(predictions[valid_mask], 99)\n",
    "    else:\n",
    "        vmin, vmax = 0, 100\n",
    "    \n",
    "    try:\n",
    "        if visualization_type == \"heatmap\":\n",
    "            # Create heatmap visualization\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            \n",
    "            im = ax.imshow(masked_predictions, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "            cbar = fig.colorbar(im, ax=ax, label='Biomass (Mg/ha)')\n",
    "            \n",
    "            ax.set_title('Predicted Above-Ground Biomass')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Add statistics as text\n",
    "            if \"statistics\" in result:\n",
    "                stats = result[\"statistics\"]\n",
    "                stats_text = (\n",
    "                    f\"Mean: {stats['mean']:.2f} Mg/ha\\n\"\n",
    "                    f\"Range: {stats['min']:.2f} - {stats['max']:.2f} Mg/ha\\n\"\n",
    "                )\n",
    "                if \"total_biomass_mg\" in stats:\n",
    "                    stats_text += f\"Total: {stats['total_biomass_mg']:.2f} Mg\"\n",
    "                \n",
    "                ax.text(0.02, 0.02, stats_text, transform=ax.transAxes,\n",
    "                       fontsize=9, verticalalignment='bottom',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "        \n",
    "        elif visualization_type == \"rgb_overlay\" and rgb_indexes is not None:\n",
    "            # Create RGB + overlay visualization\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            \n",
    "            # Try to get original image\n",
    "            if \"original_image\" in result:\n",
    "                rgb_bands = result[\"original_image\"][rgb_indexes]\n",
    "                rgb = np.dstack([rgb_bands[0], rgb_bands[1], rgb_bands[2]])\n",
    "                \n",
    "                # Normalize RGB for display\n",
    "                rgb_min = np.nanpercentile(rgb, 2)\n",
    "                rgb_max = np.nanpercentile(rgb, 98)\n",
    "                rgb_norm = np.clip((rgb - rgb_min) / (rgb_max - rgb_min), 0, 1)\n",
    "                \n",
    "                # Show RGB\n",
    "                ax.imshow(rgb_norm)\n",
    "                \n",
    "                # Create mask for overlay (where we have predictions)\n",
    "                mask = valid_mask & ~np.isclose(predictions, 0)\n",
    "                overlay = np.zeros((*predictions.shape, 4))\n",
    "                \n",
    "                # Create colormap for biomass\n",
    "                norm = plt.cm.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "                cmap = plt.cm.viridis\n",
    "                \n",
    "                # Apply colormap\n",
    "                overlay[..., :3] = cmap(norm(predictions))[..., :3]\n",
    "                overlay[..., 3] = np.where(mask, 0.7, 0)  # Set alpha channel\n",
    "                \n",
    "                ax.imshow(overlay)\n",
    "                cbar = fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap), \n",
    "                                   ax=ax, label='Biomass (Mg/ha)')\n",
    "                \n",
    "                ax.set_title('Biomass Prediction Overlay')\n",
    "            else:\n",
    "                # Fallback to heatmap if no original image\n",
    "                im = ax.imshow(masked_predictions, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "                cbar = fig.colorbar(im, ax=ax, label='Biomass (Mg/ha)')\n",
    "                ax.set_title('Predicted Above-Ground Biomass')\n",
    "            \n",
    "            ax.axis('off')\n",
    "        \n",
    "        else:\n",
    "            # Default visualization\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            im = ax.imshow(masked_predictions, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "            cbar = fig.colorbar(im, ax=ax, label='Biomass (Mg/ha)')\n",
    "            ax.set_title('Predicted Above-Ground Biomass')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Save figure to BytesIO buffer\n",
    "        buf = BytesIO()\n",
    "        fig.savefig(buf, format='png', dpi=dpi, bbox_inches='tight')\n",
    "        buf.seek(0)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return buf\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Return error image\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.text(0.5, 0.5, f\"Visualization error: {e}\", \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax.axis('off')\n",
    "        buf = BytesIO()\n",
    "        fig.savefig(buf, format='png', dpi=dpi, bbox_inches='tight')\n",
    "        buf.seek(0)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return buf\n",
    "\n",
    "# ## 7. Testing with Sample Data\n",
    "\n",
    "def test_prediction_pipeline(model_dir, test_file_path):\n",
    "    \"\"\"\n",
    "    Test the prediction pipeline with a sample file.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory containing model files\n",
    "        test_file_path (str): Path to a test GeoTIFF file\n",
    "    \"\"\"\n",
    "    print(f\"Testing prediction pipeline...\")\n",
    "    print(f\"Model directory: {model_dir}\")\n",
    "    print(f\"Test file: {test_file_path}\")\n",
    "    \n",
    "    # Load model\n",
    "    model_result = load_model(model_dir)\n",
    "    \n",
    "    if model_result['model'] is None:\n",
    "        print(\"Failed to load model\")\n",
    "        return\n",
    "    \n",
    "    # Run prediction\n",
    "    result = predict_biomass(\n",
    "        test_file_path, \n",
    "        model_result,\n",
    "        use_advanced_indices=True,\n",
    "        use_texture_features=True,\n",
    "        use_spatial_features=True,\n",
    "        use_pca_features=True\n",
    "    )\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"Prediction error: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nBiomass Statistics:\")\n",
    "    for key, value in result[\"statistics\"].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    print(\"\\nCreating visualization...\")\n",
    "    vis_buf = create_biomass_visualization(result, visualization_type=\"heatmap\")\n",
    "    \n",
    "    # Save visualization\n",
    "    output_dir = \"test_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, \"test_visualization.png\")\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(vis_buf.getvalue())\n",
    "    \n",
    "    print(f\"Visualization saved to {output_path}\")\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing prediction pipeline...\n",
      "Model directory: /teamspace/studios/this_studio/files/biomass_results_20250516_224934/models/StableResNet_20250516_225706\n",
      "Test file: /teamspace/studios/this_studio/torchgeo/experiments/biomass/center_crop_for_prediction.tif\n",
      "Loaded 99 feature names\n",
      "Model package loaded from /teamspace/studios/this_studio/files/biomass_results_20250516_224934/models/StableResNet_20250516_225706/model_package.pkl\n",
      "Using n_features=99 from package\n",
      "Model loaded successfully from /teamspace/studios/this_studio/files/biomass_results_20250516_224934/models/StableResNet_20250516_225706/model.pt\n",
      "Extracting features...\n",
      "Image dimensions: 512x512, 98 bands\n",
      "Calculated 7 spectral indices in 0.02s\n",
      "Extracted 0 texture features in 0.05s\n",
      "Calculated 0 spatial features in 0.01s\n",
      "PCA explained variance: 0.971\n",
      "Calculated 0 PCA features in 1.27s\n",
      "Extracted 138 total features in 1.51s\n",
      "Preparing 250745 valid pixels for prediction...\n",
      "Running model inference...\n",
      "Processed 250745/250745 pixels\n",
      "Prediction complete!\n",
      "Prediction complete in 3.79s\n",
      "Biomass range: 3.59 - 378.75 Mg/ha\n",
      "Mean biomass: 148.82 Mg/ha\n",
      "\n",
      "Biomass Statistics:\n",
      "  min: 3.5890402793884277\n",
      "  max: 378.7538146972656\n",
      "  mean: 148.82220458984375\n",
      "  median: 149.29965209960938\n",
      "  std: 9.477433204650879\n",
      "  sum: 37316424.0\n",
      "  pixel_count: 250745\n",
      "  valid_percentage: 95.65162658691406\n",
      "  area_hectares: 3.2375004852991643e-06\n",
      "  total_biomass_mg: 0.00048181196325458586\n",
      "\n",
      "Creating visualization...\n",
      "Visualization saved to test_output/test_visualization.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example test usage (uncomment to run)\n",
    "model_dir = \"/teamspace/studios/this_studio/files/biomass_results_20250516_224934/models/StableResNet_20250516_225706\"\n",
    "test_file = \"/teamspace/studios/this_studio/torchgeo/experiments/biomass/center_crop_for_prediction.tif\"\n",
    "test_result = test_prediction_pipeline(model_dir, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
